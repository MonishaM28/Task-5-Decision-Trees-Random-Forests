# ğŸŒ³ Task 5: Decision Trees & Random Forests

This project is part of the **AI & ML Internship**. The goal is to apply **tree-based models** (Decision Tree, Random Forest, and XGBoost) on the **Heart Disease dataset** and demonstrate both technical and interpretability skills.

---

## ğŸ“Œ Objective

* Build and evaluate **Decision Tree** and **Random Forest** models.
* Understand **overfitting** and how to control tree depth.
* Compare performance across models.
* Visualize trees, feature importances, and interpret predictions.

---

## ğŸ“Š Dataset

* **Dataset**: Heart Disease (from UCI repository).
* **Target Variable**: `target` â†’ (1 = disease, 0 = no disease).
* **Features**: age, sex, chest pain type, blood pressure, cholesterol, max heart rate, etc.

---

## ğŸ§ª Approach

1. **Exploratory Data Analysis (EDA)**

   * Target class balance âœ…
   * Correlation heatmap âœ…
   * Insights into risk factors.

2. **Modeling**

   * **Decision Tree** (visualized with Graphviz/dtreeviz).
   * **Random Forest** (ensemble model, reduces overfitting).
   * **XGBoost** (extra benchmark for better accuracy).

3. **Overfitting Analysis**

   * Depth tuning and learning curves.

4. **Evaluation**

   * Accuracy, Precision, Recall, F1, ROC-AUC.
   * Cross-validation (k=5).
   * Confusion matrices.

5. **Explainability**

   * Feature importance (RF vs XGBoost).
   * **SHAP values** â†’ global + local explanations.

---

## ğŸ“ˆ Results (Sample â€“ update with your runs)

| Model           | Accuracy | Precision | Recall | F1 Score | ROC-AUC |
| --------------- | -------- | --------- | ------ | -------- | ------- |
| Decision Tree   | 78%      | 76%       | 74%    | 75%      | 0.79    |
| Random Forest   | 85%      | 84%       | 83%    | 83%      | 0.89    |
| XGBoost (extra) | 87%      | 85%       | 86%    | 85%      | 0.91    |

âœ… Random Forest & XGBoost consistently outperformed a single Decision Tree.

---

## ğŸ¨ Visuals

* **EDA Plots**
  ![Target Distribution](images/eda_target_distribution.png)
  ![Correlation Heatmap](images/eda_correlation_heatmap.png)

* **Model Interpretations**
  ![Decision Tree](images/decision_tree.png)
  ![Feature Importances](images/random_forest_feature_importance.png)
  ![ROC Curve](images/roc_curve.png)
  ![SHAP Summary](images/shap_summary.png)
  ![Confusion Matrix](images/confusion_matrix_rf.png)

---

## âš™ï¸ Installation

```bash
pip install -r requirements.txt
```

For Colab users:

```bash
!apt-get install graphviz
```

---


## ğŸ“‚ Repository Structure
```
Task5_DecisionTree_RandomForest/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ heart.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Task5.ipynb
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ eda_target_distribution.png
â”‚   â”œâ”€â”€ eda_correlation_heatmap.png
â”‚   â”œâ”€â”€ decision_tree.png
â”‚   â”œâ”€â”€ random_forest_feature_importance.png
â”‚   â”œâ”€â”€ roc_curve.png
â”‚   â”œâ”€â”€ shap_summary.png
â”‚   â””â”€â”€ confusion_matrix_rf.png
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ† Conclusion

* Decision Trees are simple but prone to overfitting.
* Random Forests improve generalization significantly.
* XGBoost gives an extra boost in performance.
* SHAP adds **interpretability**, making the model **explainable**.

This project showcases **modeling + explainability**.
